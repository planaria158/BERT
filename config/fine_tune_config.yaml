
model_params:
  n_embd: 192
  regress_head_drop : 0.1
  vocab_size : 24      # 20 amino acids plus, X (unknown aa), MASK, CLS, and PAD tokens
  block_size : 256     # max 255 amino acids sequence and +1 for mask token
  mask_prob : 0.0      # don't mask for fine tuning
  weight_decay : 0.1
  learning_rate : 0.0001
  lr_gamma: 0.9985     # for exponential learning rate decay
  betas : [0.9, 0.95]
  accelerator: 'gpu'
  devices: 2
  batch_size: 512
  num_workers: 20
  grad_norm_clip : 1.0
  num_epochs : 100
  checkpoint_every_n_train_steps : 100
  save_top_k : 1
  monitor: 'loss'
  mode: 'min'
  log_dir: './lightning_logs/'
  log_every_nsteps: 10

  # checkpoint for the pre-trained bert model
  checkpoint_pretrained: '/home/mark/dev/myBERT/lightning_logs/pre-train/checkpoints/epoch=2-step=116800.ckpt'

  # checkpoint for entire bert + regress head model
  checkpoint_name: None #'/home/mark/dev/myBERT/lightning_logs/version_2/checkpoints/epoch=61-step=2400.ckpt'

  seed : 3407


