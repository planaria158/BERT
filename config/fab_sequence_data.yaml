
model_params:
  n_embd: 64
  n_layer: 6
  n_head : 8
  embd_pdrop : 0.1
  resid_pdrop : 0.1
  attn_pdrop : 0.1
  model_type : 'bert'
  vocab_size : 21      # 20 amino acids + 1 for mask token
  block_size : 246     # sequence length of any given entry in the dataset
  mask_prob : 0.15
  weight_decay : 0.1
  learning_rate : 0.0001
  lr_gamma: 0.9985  # for exponential learning rate decay
  betas : [0.9, 0.95]
  accelerator: 'gpu'
  devices: 1
  batch_size: 256
  num_workers: 5
  grad_norm_clip : 1.0
  max_iters : 20000
  generate_every : 100
  work_dir : './out/alphaseq'
  checkpoint_every_n_epochs : 100
  save_top_k : 1
  monitor: 'loss'
  mode: 'min'
  log_dir: './lightning_logs/'
  seed : 3407

