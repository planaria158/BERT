{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### Some BERT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_embd': 64, 'n_layer': 6, 'n_head': 8, 'embd_pdrop': 0.1, 'resid_pdrop': 0.1, 'attn_pdrop': 0.1, 'model_type': 'bert', 'vocab_size': 21, 'block_size': 246, 'mask_prob': 0.15, 'weight_decay': 0.1, 'learning_rate': 0.0001, 'lr_gamma': 0.9985, 'betas': [0.9, 0.95], 'accelerator': 'gpu', 'devices': 1, 'batch_size': 256, 'num_workers': 20, 'grad_norm_clip': 1.0, 'num_epochs': 100, 'checkpoint_every_n_train_steps': 100, 'save_top_k': 1, 'monitor': 'loss', 'mode': 'min', 'log_dir': './lightning_logs/', 'log_every_nsteps': 1000, 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/fab_sequence_data.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "type_given = False #config['model_type'] is not None\n",
    "print(type_given)\n",
    "params_given = all([config['n_layer'] is not None, config['n_head'] is not None, config['n_embd'] is not None])\n",
    "print(params_given)\n",
    "\n",
    "assert type_given ^ params_given # exactly one of these (XOR)\n",
    "print(params_given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['n_embd'] % config['n_head'] == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config['attn_pdrop'])\n",
    "        self.resid_dropout = nn.Dropout(config['resid_pdrop'])\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
    "                                     .view(1, 1, config['block_size'], config['block_size'])) # Not needed (GPT)\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "        self.block_size = config['block_size']\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask (B (batch_size) x T (seq_len))\n",
    "\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2) # BUG LEAKING???\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        #att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # GPT directional masking\n",
    "\n",
    "        if mask is not None:\n",
    "            att = att.masked_fill(mask[:, None, None, :] != 0, float('-inf')) # BERT-style masking\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
    "            c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config['resid_pdrop']),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.attn(self.ln_1(x), mask)\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\" BERT Language Model \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['vocab_size'] is not None\n",
    "        assert config['block_size'] is not None\n",
    "        self.block_size = config['block_size']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        print('block_size:', self.block_size)\n",
    "        print('vocab_size:', self.vocab_size)\n",
    "        type_given = config['model_type'] is not None\n",
    "        params_given = all([config['n_layer'] is not None, config['n_head'] is not None, config['n_embd'] is not None])\n",
    "\n",
    "        # assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        # if type_given:\n",
    "        #     # translate from model_type to detailed configuration\n",
    "        #     config['merge_from_dict']({\n",
    "        #         # names follow the huggingface naming conventions\n",
    "        #         # GPT-1 yer=12, n_head=12, n_embd=768),  # 117M params\n",
    "        #         # GPT-2 configs\n",
    "        #         'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "        #         'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "        #         'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "        #         'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        #         # Gophers\n",
    "        #         'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "        #         # (there are a number more...)\n",
    "        #         # I made these tiny models up\n",
    "        #         'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
    "        #         'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "        #         'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "        #     }[config['model_type']])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
    "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
    "            drop = nn.Dropout(config['embd_pdrop']),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
    "            ln_f = nn.LayerNorm(config['n_embd']),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    # @classmethod\n",
    "    # def from_pretrained(cls, model_type):\n",
    "    #     \"\"\"\n",
    "    #     Initialize a pretrained GPT model by copying over the weights\n",
    "    #     from a huggingface/transformers checkpoint.\n",
    "    #     \"\"\"\n",
    "    #     assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "    #     from transformers import GPT2LMHeadModel\n",
    "\n",
    "    #     # create a from-scratch initialized minGPT model\n",
    "    #     config = cls.get_default_config()\n",
    "    #     config['model_type'] = model_type\n",
    "    #     config['vocab_size'] = 50257 # openai's model vocabulary\n",
    "    #     config['block_size'] = 1024  # openai's model block_size\n",
    "    #     model = BERT(config)\n",
    "    #     sd = model.state_dict()\n",
    "\n",
    "    #     # init a huggingface/transformers model\n",
    "    #     model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    #     sd_hf = model_hf.state_dict()\n",
    "\n",
    "    #     # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "    #     keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
    "    #     transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "    #     # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
    "    #     # this means that we have to transpose these weights when we import them\n",
    "    #     assert len(keys) == len(sd)\n",
    "    #     for k in keys:\n",
    "    #         if any(k.endswith(w) for w in transposed):\n",
    "    #             # special treatment for the Conv1D weights we need to transpose\n",
    "    #             assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "    #             with torch.no_grad():\n",
    "    #                 sd[k].copy_(sd_hf[k].t())\n",
    "    #         else:\n",
    "    #             # vanilla copy over the other parameters\n",
    "    #             assert sd_hf[k].shape == sd[k].shape\n",
    "    #             with torch.no_grad():\n",
    "    #                 sd[k].copy_(sd_hf[k])\n",
    "\n",
    "    #     return model\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config['weight_decay']},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config['learning_rate'], betas=train_config['betas'])\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, mask)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        idx = idx.view(-1)\n",
    "\n",
    "        # Run in Masked Language Model mode\n",
    "        if mask is not None:\n",
    "            mask = mask.view(-1)\n",
    "            mask_idx = torch.nonzero(mask)\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size),  mask, reduction='none')\n",
    "            loss = loss.sum() / mask_idx.shape[0]\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, mask_token, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size - 1 else idx[:, -self.block_size+1:]\n",
    "\n",
    "            mask = torch.cat((torch.zeros_like(idx_cond).to(device), torch.tensor([[mask_token]]).to(device)), 1)\n",
    "            idx_cond = torch.cat((idx_cond, torch.tensor([[mask_token]]).to(device)), 1)\n",
    "\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond, mask)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FABSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows):\n",
    "        self.config = config\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # my_set = set()   \n",
    "        # def make_set(x):\n",
    "        #     for c in x:\n",
    "        #         my_set.add(c)\n",
    "\n",
    "        # self.df['Sequence'].apply(make_set)\n",
    "        # self.chars = sorted(list(my_set)) + [\"[MASK]\"]\n",
    "        # print('len of chars:', len(self.chars))\n",
    "        # print('chars:', self.chars)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        self.chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        chunk = self.df.loc[idx, 'Sequence']\n",
    "        \n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: ./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv\n",
      "vocabulary: ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
      "data has 1903921 rows, 21 vocab size (unique).\n",
      "1903921\n",
      "config[vocab_size]: 21 , config[block_size]: 246\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "train_dataset = FABSequenceDataset(config, data_path, skiprows=6)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "config['block_size'] = train_dataset.get_block_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "len = train_dataset.__len__()\n",
    "\n",
    "# setup the dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    # sampler=torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=int(len)),\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Use pytorch lightning to manage the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Pytorch Lightning Module that hosts the BERT model\n",
    "# and runs the training, validation, and testing loops\n",
    "#----------------------------------------------------------\n",
    "class BERT_Lightning(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(BERT_Lightning, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = BERT(config)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return self.model(x, mask)\n",
    "\n",
    "    def common_forward(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        logits, loss = model(x, mask)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_norm_clip'])\n",
    "        return logits, loss\n",
    "        # y_hat = torch.squeeze(y_hat)\n",
    "        # loss = self.criterion(y_hat, y)\n",
    "        # return loss, y_hat, y, transformer_out, attns, trans_input\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, loss = self.common_forward(batch, batch_idx)\n",
    "        self.log_dict({\"loss\": loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "   \n",
    "    # def on_validation_start(self):\n",
    "    #     self.y_preds = []\n",
    "    #     self.y_true = []\n",
    "    #     self.metrics = None\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     val_loss, y_hat, y, _, _, _ = self.common_forward(batch, batch_idx)\n",
    "    #     self.y_true.extend(y.cpu().numpy().tolist())\n",
    "    #     self.y_preds.extend(y_hat.cpu().numpy().tolist())\n",
    "    #     self.log_dict({\"val_loss\": val_loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "    #     self.logger.experiment.add_scalars('loss', {'valid': val_loss}, self.global_step)\n",
    "    #     return val_loss\n",
    "    \n",
    "    # def on_validation_end(self):\n",
    "    #     assert(len(self.y_preds) == len(self.y_true))\n",
    "    #     self.metrics = self._get_avm_metrics(self.y_true, self.y_preds)\n",
    "    #     mape_metrics = {'MAPE':self.metrics['MAPE'], 'mdAPE':self.metrics['mdAPE']}\n",
    "    #     ppe_metrics = {'PPE10':self.metrics['PPE10'], 'PPE20':self.metrics['PPE20']}\n",
    "    #     self.logger.experiment.add_scalars('mape', mape_metrics, self.current_epoch)\n",
    "    #     self.logger.experiment.add_scalars('ppe', ppe_metrics, self.current_epoch)\n",
    "    #     self.y_preds = None\n",
    "    #     self.y_true = None\n",
    "    #     return \n",
    "\n",
    "    # def on_test_start(self):\n",
    "    #     self.y_preds = []\n",
    "    #     self.y_true = []\n",
    "    #     self.metrics = None\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     test_loss, y_hat, y, transformer_out, attns, trans_input = self.common_forward(batch, batch_idx)\n",
    "    #     self.log_dict({\"test_loss\": test_loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "    #     self.logger.experiment.add_scalars('loss', {'test': test_loss},self.global_step)\n",
    "    #     self.y_true.extend(y.cpu().numpy().tolist())\n",
    "    #     self.y_preds.extend(y_hat.cpu().numpy().tolist())\n",
    "    #     return test_loss\n",
    "\n",
    "    # def on_test_end(self):\n",
    "    #     assert(len(self.y_preds) == len(self.y_true))\n",
    "    #     self.metrics = self._get_avm_metrics(self.y_true, self.y_preds)\n",
    "    #     print(self.metrics)\n",
    "    #     return \n",
    "    \n",
    "    # Function to get evaluation metrics (MAPE, MdAPE, PPE10, PPE20)\n",
    "    # def _get_avm_metrics(self, y_true, y_pred):\n",
    "    #     y_true = np.exp(np.array(y_true))  # convert back to house prices.\n",
    "    #     y_pred = np.exp(np.array(y_pred))\n",
    "    #     ppe10 = np.count_nonzero((np.abs((np.divide(y_pred, y_true)) - 1) <= 0.1)) / y_pred.size\n",
    "    #     ppe20 = np.count_nonzero((np.abs((np.divide(y_pred, y_true)) - 1) <= 0.2)) / y_pred.size\n",
    "    #     mape  = (np.sum(np.abs(y_true - y_pred) / y_true))/len(y_true)\n",
    "    #     mdape = np.median(np.abs(y_true - y_pred)/y_true)\n",
    "\n",
    "    #     return {'MAPE': mape, 'mdAPE': mdape,\n",
    "    #             'PPE10': ppe10, 'PPE20': ppe20,\n",
    "    #             'Sample_Size': y_true.shape[0]}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.config['learning_rate']\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), betas=config['betas'], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config['lr_gamma'])\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_size: 246\n",
      "vocab_size: 21\n",
      "number of parameters: 0.32M\n",
      "Model has: 318464 parameters\n"
     ]
    }
   ],
   "source": [
    "model = BERT_Lightning(config) \n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print('Model has:', int(total_params), 'parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2024-04-20 20:43:27.404596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-20 20:43:27.404630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-20 20:43:27.405768: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-20 20:43:27.412181: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 20:43:28.312312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | BERT    | 318 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.274     Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with signal SIGSEGV",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing\u001b[39m\u001b[38;5;124m'\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp_notebook\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m#strategy='ddp', #'ddp_find_unused_parameters_true', \u001b[39;00m\n\u001b[1;32m     17\u001b[0m                         accelerator\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     18\u001b[0m                         devices\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevices\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                         log_every_n_steps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_every_nsteps\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     22\u001b[0m                         callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback])   \n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:140\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exitcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     name \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mSignals(\u001b[38;5;241m-\u001b[39mexitcode)\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    142\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    143\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    144\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    145\u001b[0m         signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    150\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    151\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    152\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    153\u001b[0m     )\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with signal SIGSEGV"
     ]
    }
   ],
   "source": [
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Training\n",
    "#--------------------------------------------------------------------\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=config['save_top_k'],\n",
    "    every_n_train_steps=config['checkpoint_every_n_train_steps'],\n",
    "    save_on_train_epoch_end=True,\n",
    "    monitor = config['monitor'],\n",
    "    mode = config['mode']\n",
    ")\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=config['log_dir'], default_hp_metric=False)\n",
    "\n",
    "print('Using', config['accelerator'])\n",
    "trainer = pl.Trainer(#strategy='ddp_notebook', #strategy='ddp', #'ddp_find_unused_parameters_true', \n",
    "                        accelerator=config['accelerator'], \n",
    "                        devices=config['devices'],\n",
    "                        max_epochs=config['num_epochs'],   \n",
    "                        logger=logger, \n",
    "                        log_every_n_steps=config['log_every_nsteps'], \n",
    "                        callbacks=[checkpoint_callback])   \n",
    "\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
