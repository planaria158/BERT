{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### Some BERT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_embd': 64, 'n_layer': 6, 'n_head': 8, 'embd_pdrop': 0.1, 'resid_pdrop': 0.1, 'attn_pdrop': 0.1, 'model_type': 'bert', 'vocab_size': 21, 'block_size': 246, 'mask_prob': 0.15, 'weight_decay': 0.1, 'learning_rate': 0.0001, 'betas': [0.9, 0.95], 'accelerator': 'gpu', 'devices': 2, 'batch_size': 256, 'num_workers': 5, 'grad_norm_clip': 1.0, 'max_iters': 20000, 'generate_every': 100, 'work_dir': './out/alphaseq', 'checkpoint_every_n_epochs': 100, 'save_top_k': 1, 'monitor': 'loss', 'mode': 'min', 'log_dir': './lightning_logs/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/fab_sequence_data.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "type_given = False #config['model_type'] is not None\n",
    "print(type_given)\n",
    "params_given = all([config['n_layer'] is not None, config['n_head'] is not None, config['n_embd'] is not None])\n",
    "print(params_given)\n",
    "\n",
    "assert type_given ^ params_given # exactly one of these (XOR)\n",
    "print(params_given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['n_embd'] % config['n_head'] == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config['attn_pdrop'])\n",
    "        self.resid_dropout = nn.Dropout(config['resid_pdrop'])\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
    "                                     .view(1, 1, config['block_size'], config['block_size'])) # Not needed (GPT)\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "        self.block_size = config['block_size']\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask (B (batch_size) x T (seq_len))\n",
    "\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2) # BUG LEAKING???\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        #att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # GPT directional masking\n",
    "\n",
    "        if mask is not None:\n",
    "            att = att.masked_fill(mask[:, None, None, :] != 0, float('-inf')) # BERT-style masking\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
    "            c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config['resid_pdrop']),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.attn(self.ln_1(x), mask)\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\" BERT Language Model \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['vocab_size'] is not None\n",
    "        assert config['block_size'] is not None\n",
    "        self.block_size = config['block_size']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        print('block_size:', self.block_size)\n",
    "        print('vocab_size:', self.vocab_size)\n",
    "        type_given = config['model_type'] is not None\n",
    "        params_given = all([config['n_layer'] is not None, config['n_head'] is not None, config['n_embd'] is not None])\n",
    "\n",
    "        # assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        # if type_given:\n",
    "        #     # translate from model_type to detailed configuration\n",
    "        #     config['merge_from_dict']({\n",
    "        #         # names follow the huggingface naming conventions\n",
    "        #         # GPT-1 yer=12, n_head=12, n_embd=768),  # 117M params\n",
    "        #         # GPT-2 configs\n",
    "        #         'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "        #         'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "        #         'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "        #         'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        #         # Gophers\n",
    "        #         'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "        #         # (there are a number more...)\n",
    "        #         # I made these tiny models up\n",
    "        #         'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
    "        #         'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "        #         'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "        #     }[config['model_type']])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
    "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
    "            drop = nn.Dropout(config['embd_pdrop']),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
    "            ln_f = nn.LayerNorm(config['n_embd']),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    # @classmethod\n",
    "    # def from_pretrained(cls, model_type):\n",
    "    #     \"\"\"\n",
    "    #     Initialize a pretrained GPT model by copying over the weights\n",
    "    #     from a huggingface/transformers checkpoint.\n",
    "    #     \"\"\"\n",
    "    #     assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "    #     from transformers import GPT2LMHeadModel\n",
    "\n",
    "    #     # create a from-scratch initialized minGPT model\n",
    "    #     config = cls.get_default_config()\n",
    "    #     config['model_type'] = model_type\n",
    "    #     config['vocab_size'] = 50257 # openai's model vocabulary\n",
    "    #     config['block_size'] = 1024  # openai's model block_size\n",
    "    #     model = BERT(config)\n",
    "    #     sd = model.state_dict()\n",
    "\n",
    "    #     # init a huggingface/transformers model\n",
    "    #     model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    #     sd_hf = model_hf.state_dict()\n",
    "\n",
    "    #     # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "    #     keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
    "    #     transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "    #     # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
    "    #     # this means that we have to transpose these weights when we import them\n",
    "    #     assert len(keys) == len(sd)\n",
    "    #     for k in keys:\n",
    "    #         if any(k.endswith(w) for w in transposed):\n",
    "    #             # special treatment for the Conv1D weights we need to transpose\n",
    "    #             assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "    #             with torch.no_grad():\n",
    "    #                 sd[k].copy_(sd_hf[k].t())\n",
    "    #         else:\n",
    "    #             # vanilla copy over the other parameters\n",
    "    #             assert sd_hf[k].shape == sd[k].shape\n",
    "    #             with torch.no_grad():\n",
    "    #                 sd[k].copy_(sd_hf[k])\n",
    "\n",
    "    #     return model\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config['weight_decay']},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config['learning_rate'], betas=train_config['betas'])\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, mask)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        idx = idx.view(-1)\n",
    "\n",
    "        # Run in Masked Language Model mode\n",
    "        if mask is not None:\n",
    "            mask = mask.view(-1)\n",
    "            mask_idx = torch.nonzero(mask)\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size),  mask, reduction='none')\n",
    "            loss = loss.sum() / mask_idx.shape[0]\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, mask_token, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size - 1 else idx[:, -self.block_size+1:]\n",
    "\n",
    "            mask = torch.cat((torch.zeros_like(idx_cond).to(device), torch.tensor([[mask_token]]).to(device)), 1)\n",
    "            idx_cond = torch.cat((idx_cond, torch.tensor([[mask_token]]).to(device)), 1)\n",
    "\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond, mask)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FABSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows):\n",
    "        self.config = config\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # my_set = set()   \n",
    "        # def make_set(x):\n",
    "        #     for c in x:\n",
    "        #         my_set.add(c)\n",
    "\n",
    "        # self.df['Sequence'].apply(make_set)\n",
    "        # self.chars = sorted(list(my_set)) + [\"[MASK]\"]\n",
    "        # print('len of chars:', len(self.chars))\n",
    "        # print('chars:', self.chars)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        self.chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        chunk = self.df.loc[idx, 'Sequence']\n",
    "        \n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "# train_dataset = FABSequenceDataset(config, data_path, skiprows=6)\n",
    "# print(train_dataset.__len__())\n",
    "# dix, mask = train_dataset.__getitem__(0)\n",
    "# print(dix)\n",
    "# print()\n",
    "# print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CharDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Emits batches of characters\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config, data):\n",
    "#         self.config = config\n",
    "#         chars = sorted(list(set(data))) + [\"[MASK]\"] # + mask token\n",
    "#         data_size, vocab_size = len(data), len(chars)\n",
    "#         print('data has %d characters, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "#         self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "#         self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.data = data\n",
    "\n",
    "#     def get_vocab_size(self):\n",
    "#         return self.vocab_size\n",
    "\n",
    "#     def get_block_size(self):\n",
    "#         return self.config['block_size']\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data) - self.config['block_size']\n",
    "\n",
    "#     \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "#     def __getitem__(self, idx):\n",
    "#         # grab a chunk of (block_size) characters from the data\n",
    "#         chunk = self.data[idx:idx + self.config['block_size']]\n",
    "#         # encode every character to an integer\n",
    "#         dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "#         # get number of tokens to mask\n",
    "#         n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "#         # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "#         masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "#         mask = torch.zeros_like(dix)\n",
    "\n",
    "#         # copy the actual tokens to the mask\n",
    "#         mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "#         # ... and overwrite then with MASK token in the data\n",
    "#         dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "#         return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open('./minBERT/input.txt', 'r').read() \n",
    "# train_dataset = CharDataset(config, text)\n",
    "# print(train_dataset.__len__())\n",
    "# dix, mask = train_dataset.__getitem__(0)\n",
    "# print(dix)\n",
    "# print()\n",
    "# print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "# from minbert.utils import CfgNode as CN\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config, model, train_dataset):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "\n",
    "        # determine the device we'll train on\n",
    "        if config['device'] == 'auto':\n",
    "            self.device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config['device']\n",
    "        print(\"running on device\", self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=config['batch_size'],\n",
    "            num_workers=config['num_workers'],\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        data_iter = iter(train_loader)\n",
    "        while True:\n",
    "            # fetch the next batch (x, y) and re-init iterator if needed\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            x, mask = batch\n",
    "\n",
    "            # forward the model\n",
    "            logits, self.loss = model(x, mask)\n",
    "\n",
    "            # backprop and update the parameters\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            self.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_norm_clip'])\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            self.iter_num += 1\n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            # termination conditions\n",
    "            if config['max_iters'] is not None and self.iter_num >= config['max_iters']:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: ./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv\n",
      "vocabulary: ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
      "data has 1903921 rows, 21 vocab size (unique).\n",
      "1903921\n",
      "config[vocab_size]: 21 , config[block_size]: 246\n"
     ]
    }
   ],
   "source": [
    "# construct the training dataset\n",
    "# text = open('./minBERT/input.txt', 'r').read() \n",
    "# train_dataset = CharDataset(config, text)\n",
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "train_dataset = FABSequenceDataset(config, data_path, skiprows=6)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "config['block_size'] = train_dataset.get_block_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "len = train_dataset.__len__()\n",
    "\n",
    "# setup the dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=int(len)),\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct the model\n",
    "# # model = BERT(config) #.model)\n",
    "\n",
    "# chkpoint_path = './out/alphaseq/model.pt'\n",
    "# print('loading the model from checkpoint:', chkpoint_path)\n",
    "# model = BERT(config)\n",
    "# model.load_state_dict(torch.load(chkpoint_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get default config and overrides from the command line, if any\n",
    "# config = get_config()\n",
    "# config.merge_from_args(sys.argv[1:])\n",
    "\n",
    "\n",
    "from minBERT.minbert.utils import setup_logging, set_seed\n",
    "print(config)\n",
    "# setup_logging(config)\n",
    "set_seed(config['seed'])\n",
    "\n",
    "# construct the trainer object\n",
    "trainer = Trainer(config, model, train_dataset)\n",
    "\n",
    "# iteration callback\n",
    "def batch_end_callback(trainer):\n",
    "\n",
    "    if trainer.iter_num % 10 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "\n",
    "    if (trainer.iter_num > 0) and (trainer.iter_num % config['generate_every'] == 0):\n",
    "        # # evaluate both the train and test score\n",
    "        # model.eval()\n",
    "        # with torch.no_grad():\n",
    "        #     # sample from the model...\n",
    "        #     context = \"O God, O God!\"\n",
    "        #     x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "        #     y = model.generate(x, 500, mask_token=train_dataset.stoi[\"[MASK]\"], temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "        #     completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "        #     print(completion)\n",
    "            \n",
    "        # save the latest model\n",
    "        print(\"saving model\")\n",
    "        ckpt_path = os.path.join(config['work_dir'], \"model.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        # revert model to training mode\n",
    "        model.train()\n",
    "\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimization\n",
    "print('calling trainer.run()')\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Use pytorch lightning to manage the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Pytorch Lightning Module that hosts the BERT model\n",
    "# and runs the training, validation, and testing loops\n",
    "#----------------------------------------------------------\n",
    "class BERT_Lightning(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(BERT_Lightning, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = BERT(config)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return self.model(x, mask)\n",
    "\n",
    "    def common_forward(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        logits, loss = model(x, mask)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_norm_clip'])\n",
    "        return logits, loss\n",
    "        # y_hat = torch.squeeze(y_hat)\n",
    "        # loss = self.criterion(y_hat, y)\n",
    "        # return loss, y_hat, y, transformer_out, attns, trans_input\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, loss = self.common_forward(batch, batch_idx)\n",
    "        self.log_dict({\"loss\": loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "   \n",
    "    # def on_validation_start(self):\n",
    "    #     self.y_preds = []\n",
    "    #     self.y_true = []\n",
    "    #     self.metrics = None\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     val_loss, y_hat, y, _, _, _ = self.common_forward(batch, batch_idx)\n",
    "    #     self.y_true.extend(y.cpu().numpy().tolist())\n",
    "    #     self.y_preds.extend(y_hat.cpu().numpy().tolist())\n",
    "    #     self.log_dict({\"val_loss\": val_loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "    #     self.logger.experiment.add_scalars('loss', {'valid': val_loss}, self.global_step)\n",
    "    #     return val_loss\n",
    "    \n",
    "    # def on_validation_end(self):\n",
    "    #     assert(len(self.y_preds) == len(self.y_true))\n",
    "    #     self.metrics = self._get_avm_metrics(self.y_true, self.y_preds)\n",
    "    #     mape_metrics = {'MAPE':self.metrics['MAPE'], 'mdAPE':self.metrics['mdAPE']}\n",
    "    #     ppe_metrics = {'PPE10':self.metrics['PPE10'], 'PPE20':self.metrics['PPE20']}\n",
    "    #     self.logger.experiment.add_scalars('mape', mape_metrics, self.current_epoch)\n",
    "    #     self.logger.experiment.add_scalars('ppe', ppe_metrics, self.current_epoch)\n",
    "    #     self.y_preds = None\n",
    "    #     self.y_true = None\n",
    "    #     return \n",
    "\n",
    "    # def on_test_start(self):\n",
    "    #     self.y_preds = []\n",
    "    #     self.y_true = []\n",
    "    #     self.metrics = None\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     test_loss, y_hat, y, transformer_out, attns, trans_input = self.common_forward(batch, batch_idx)\n",
    "    #     self.log_dict({\"test_loss\": test_loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "    #     self.logger.experiment.add_scalars('loss', {'test': test_loss},self.global_step)\n",
    "    #     self.y_true.extend(y.cpu().numpy().tolist())\n",
    "    #     self.y_preds.extend(y_hat.cpu().numpy().tolist())\n",
    "    #     return test_loss\n",
    "\n",
    "    # def on_test_end(self):\n",
    "    #     assert(len(self.y_preds) == len(self.y_true))\n",
    "    #     self.metrics = self._get_avm_metrics(self.y_true, self.y_preds)\n",
    "    #     print(self.metrics)\n",
    "    #     return \n",
    "    \n",
    "    # Function to get evaluation metrics (MAPE, MdAPE, PPE10, PPE20)\n",
    "    # def _get_avm_metrics(self, y_true, y_pred):\n",
    "    #     y_true = np.exp(np.array(y_true))  # convert back to house prices.\n",
    "    #     y_pred = np.exp(np.array(y_pred))\n",
    "    #     ppe10 = np.count_nonzero((np.abs((np.divide(y_pred, y_true)) - 1) <= 0.1)) / y_pred.size\n",
    "    #     ppe20 = np.count_nonzero((np.abs((np.divide(y_pred, y_true)) - 1) <= 0.2)) / y_pred.size\n",
    "    #     mape  = (np.sum(np.abs(y_true - y_pred) / y_true))/len(y_true)\n",
    "    #     mdape = np.median(np.abs(y_true - y_pred)/y_true)\n",
    "\n",
    "    #     return {'MAPE': mape, 'mdAPE': mdape,\n",
    "    #             'PPE10': ppe10, 'PPE20': ppe20,\n",
    "    #             'Sample_Size': y_true.shape[0]}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.config['learning_rate']\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), betas=config['betas'], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config['lr_gamma'])\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_size: 246\n",
      "vocab_size: 21\n",
      "number of parameters: 0.32M\n",
      "Model has: 318464 parameters\n"
     ]
    }
   ],
   "source": [
    "model = BERT_Lightning(config) \n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print('Model has:', int(total_params), 'parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2024-04-20 20:08:18.522146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-20 20:08:18.522171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-20 20:08:18.523303: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-20 20:08:18.529325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 20:08:19.251130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 963, in _run\n    self.strategy.setup(self)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 174, in setup\n    self.setup_optimizers(trainer)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 139, in setup_optimizers\n    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py\", line 178, in _init_optimizers_and_lr_schedulers\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 157, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/tmp/ipykernel_1163947/4246104323.py\", line 89, in configure_optimizers\n    lr = self.train_config['learning_rate']\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1688, in __getattr__\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\nAttributeError: 'BERT_Lightning' object has no attribute 'train_config'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing\u001b[39m\u001b[38;5;124m'\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;66;03m#strategy='ddp_notebook', \u001b[39;00m\n\u001b[1;32m     16\u001b[0m                         accelerator\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     17\u001b[0m                         devices\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevices\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:158\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    157\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 963, in _run\n    self.strategy.setup(self)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 174, in setup\n    self.setup_optimizers(trainer)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 139, in setup_optimizers\n    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py\", line 178, in _init_optimizers_and_lr_schedulers\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 157, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/tmp/ipykernel_1163947/4246104323.py\", line 89, in configure_optimizers\n    lr = self.train_config['learning_rate']\n  File \"/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1688, in __getattr__\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\nAttributeError: 'BERT_Lightning' object has no attribute 'train_config'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Training\n",
    "#--------------------------------------------------------------------\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=config['save_top_k'],\n",
    "    every_n_epochs=config['checkpoint_every_n_epochs'],\n",
    "    monitor = config['monitor'],\n",
    "    mode = config['mode']\n",
    ")\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=config['log_dir'], default_hp_metric=False)\n",
    "\n",
    "print('Using', config['accelerator'])\n",
    "trainer = pl.Trainer(#strategy='ddp_notebook', \n",
    "                        accelerator=config['accelerator'], \n",
    "                        devices=config['devices']) \n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
