{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc code taken from public sources to make a BERT implementation for protein-protein binding studies\n",
    "\n",
    "* https://github.com/barneyhill/minBERT\n",
    "* https://github.com/karpathy/minGPT\n",
    "\n",
    "protein sequence data taken from:\n",
    "* https://www.nature.com/articles/s41467-023-39022-2\n",
    "* https://zenodo.org/records/7783546\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### Some BERT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/fab_sequence_data.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_given = False #config['model_type'] is not None\n",
    "print(type_given)\n",
    "params_given = all([config['n_layer'] is not None, config['n_head'] is not None, config['n_embd'] is not None])\n",
    "print(params_given)\n",
    "\n",
    "assert type_given ^ params_given # exactly one of these (XOR)\n",
    "print(params_given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['n_embd'] % config['n_head'] == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config['attn_pdrop'])\n",
    "        self.resid_dropout = nn.Dropout(config['resid_pdrop'])\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
    "                                     .view(1, 1, config['block_size'], config['block_size'])) # Not needed (GPT)\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "        self.block_size = config['block_size']\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask (B (batch_size) x T (seq_len))\n",
    "\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2) # BUG LEAKING???\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        #att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # GPT directional masking\n",
    "\n",
    "        if mask is not None:\n",
    "            att = att.masked_fill(mask[:, None, None, :] != 0, float('-inf')) # BERT-style masking\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
    "            c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config['resid_pdrop']),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.attn(self.ln_1(x), mask)\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\" BERT Language Model \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['vocab_size'] is not None\n",
    "        assert config['block_size'] is not None\n",
    "        self.block_size = config['block_size']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        print('block_size:', self.block_size)\n",
    "        print('vocab_size:', self.vocab_size)\n",
    "\n",
    "        # type_given = config['model_type'] is not None\n",
    "        # params_given = all([config['n_layer'] is not None, config['n_head'] is not None, config['n_embd'] is not None])\n",
    "        # assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        # if type_given:\n",
    "        #     # translate from model_type to detailed configuration\n",
    "        #     config['merge_from_dict']({\n",
    "        #         # names follow the huggingface naming conventions\n",
    "        #         # GPT-1 yer=12, n_head=12, n_embd=768),  # 117M params\n",
    "        #         # GPT-2 configs\n",
    "        #         'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "        #         'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "        #         'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "        #         'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        #         # Gophers\n",
    "        #         'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "        #         # (there are a number more...)\n",
    "        #         # I made these tiny models up\n",
    "        #         'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
    "        #         'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "        #         'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "        #     }[config['model_type']])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
    "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
    "            drop = nn.Dropout(config['embd_pdrop']),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
    "            ln_f = nn.LayerNorm(config['n_embd']),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    # @classmethod\n",
    "    # def from_pretrained(cls, model_type):\n",
    "    #     \"\"\"\n",
    "    #     Initialize a pretrained GPT model by copying over the weights\n",
    "    #     from a huggingface/transformers checkpoint.\n",
    "    #     \"\"\"\n",
    "    #     assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "    #     from transformers import GPT2LMHeadModel\n",
    "\n",
    "    #     # create a from-scratch initialized minGPT model\n",
    "    #     config = cls.get_default_config()\n",
    "    #     config['model_type'] = model_type\n",
    "    #     config['vocab_size'] = 50257 # openai's model vocabulary\n",
    "    #     config['block_size'] = 1024  # openai's model block_size\n",
    "    #     model = BERT(config)\n",
    "    #     sd = model.state_dict()\n",
    "\n",
    "    #     # init a huggingface/transformers model\n",
    "    #     model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    #     sd_hf = model_hf.state_dict()\n",
    "\n",
    "    #     # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "    #     keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
    "    #     transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "    #     # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
    "    #     # this means that we have to transpose these weights when we import them\n",
    "    #     assert len(keys) == len(sd)\n",
    "    #     for k in keys:\n",
    "    #         if any(k.endswith(w) for w in transposed):\n",
    "    #             # special treatment for the Conv1D weights we need to transpose\n",
    "    #             assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "    #             with torch.no_grad():\n",
    "    #                 sd[k].copy_(sd_hf[k].t())\n",
    "    #         else:\n",
    "    #             # vanilla copy over the other parameters\n",
    "    #             assert sd_hf[k].shape == sd[k].shape\n",
    "    #             with torch.no_grad():\n",
    "    #                 sd[k].copy_(sd_hf[k])\n",
    "\n",
    "    #     return model\n",
    "\n",
    "    # def configure_optimizers(self, train_config):\n",
    "    #     \"\"\"\n",
    "    #     This long function is unfortunately doing something very simple and is being very defensive:\n",
    "    #     We are separating out all parameters of the model into two buckets: those that will experience\n",
    "    #     weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "    #     We are then returning the PyTorch optimizer object.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "    #     decay = set()\n",
    "    #     no_decay = set()\n",
    "    #     whitelist_weight_modules = (torch.nn.Linear, )\n",
    "    #     blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "    #     for mn, m in self.named_modules():\n",
    "    #         for pn, p in m.named_parameters():\n",
    "    #             fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "    #             # random note: because named_modules and named_parameters are recursive\n",
    "    #             # we will see the same tensors p many many times. but doing it this way\n",
    "    #             # allows us to know which parent module any tensor p belongs to...\n",
    "    #             if pn.endswith('bias'):\n",
    "    #                 # all biases will not be decayed\n",
    "    #                 no_decay.add(fpn)\n",
    "    #             elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "    #                 # weights of whitelist modules will be weight decayed\n",
    "    #                 decay.add(fpn)\n",
    "    #             elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "    #                 # weights of blacklist modules will NOT be weight decayed\n",
    "    #                 no_decay.add(fpn)\n",
    "\n",
    "    #     # validate that we considered every parameter\n",
    "    #     param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "    #     inter_params = decay & no_decay\n",
    "    #     union_params = decay | no_decay\n",
    "    #     assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "    #     assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "    #                                                 % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "    #     # create the pytorch optimizer object\n",
    "    #     optim_groups = [\n",
    "    #         {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config['weight_decay']},\n",
    "    #         {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "    #     ]\n",
    "\n",
    "    #     optimizer = torch.optim.AdamW(optim_groups, lr=train_config['learning_rate'], betas=train_config['betas'])\n",
    "    #     return optimizer\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, mask)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        idx = idx.view(-1)\n",
    "\n",
    "        # Run in Masked Language Model mode\n",
    "        if mask is not None:\n",
    "            mask = mask.view(-1)\n",
    "            mask_idx = torch.nonzero(mask)\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size),  mask, reduction='none')\n",
    "            loss = loss.sum() / mask_idx.shape[0]\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, mask_token, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size - 1 else idx[:, -self.block_size+1:]\n",
    "\n",
    "            mask = torch.cat((torch.zeros_like(idx_cond).to(device), torch.tensor([[mask_token]]).to(device)), 1)\n",
    "            idx_cond = torch.cat((idx_cond, torch.tensor([[mask_token]]).to(device)), 1)\n",
    "\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond, mask)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FABSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows):  #pk_file_path):\n",
    "        self.config = config\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        # self.df = pk.load(open(pk_file_path, 'rb'))\n",
    "        \n",
    "        # my_set = set()   \n",
    "        # def make_set(x):\n",
    "        #     for c in x:\n",
    "        #         my_set.add(c)\n",
    "\n",
    "        # self.df['Sequence'].apply(make_set)\n",
    "        # self.chars = sorted(list(my_set)) + [\"[MASK]\"]\n",
    "        # print('len of chars:', len(self.chars))\n",
    "        # print('chars:', self.chars)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        self.chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        chunk = self.df.loc[idx, 'Sequence']\n",
    "        \n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/train_set.pkl'\n",
    "# test_data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/test_set.pkl'\n",
    "\n",
    "train_data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "train_dataset = FABSequenceDataset(config, train_data_path, skiprows=6)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "config['block_size'] = train_dataset.get_block_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "# test_dataset = FABSequenceDataset(config, test_data_path)\n",
    "# print(test_dataset.__len__())\n",
    "# print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "\n",
    "# setup the dataloaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n",
    "# test_loader = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Use pytorch lightning to manage the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Pytorch Lightning Module that hosts the BERT model\n",
    "# and runs the training, validation, and testing loops\n",
    "#----------------------------------------------------------\n",
    "class BERT_Lightning(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(BERT_Lightning, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = BERT(config)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return self.model(x, mask)\n",
    "\n",
    "    def common_forward(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        logits, loss = model(x, mask)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_norm_clip'])\n",
    "        return logits, loss\n",
    "        # y_hat = torch.squeeze(y_hat)\n",
    "        # loss = self.criterion(y_hat, y)\n",
    "        # return loss, y_hat, y, transformer_out, attns, trans_input\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, loss = self.common_forward(batch, batch_idx)\n",
    "        self.log_dict({\"loss\": loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "   \n",
    "    # def on_validation_start(self):\n",
    "    #     self.y_preds = []\n",
    "    #     self.y_true = []\n",
    "    #     self.metrics = None\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     val_loss, y_hat, y, _, _, _ = self.common_forward(batch, batch_idx)\n",
    "    #     self.y_true.extend(y.cpu().numpy().tolist())\n",
    "    #     self.y_preds.extend(y_hat.cpu().numpy().tolist())\n",
    "    #     self.log_dict({\"val_loss\": val_loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "    #     self.logger.experiment.add_scalars('loss', {'valid': val_loss}, self.global_step)\n",
    "    #     return val_loss\n",
    "    \n",
    "    # def on_validation_end(self):\n",
    "    #     assert(len(self.y_preds) == len(self.y_true))\n",
    "    #     self.metrics = self._get_avm_metrics(self.y_true, self.y_preds)\n",
    "    #     mape_metrics = {'MAPE':self.metrics['MAPE'], 'mdAPE':self.metrics['mdAPE']}\n",
    "    #     ppe_metrics = {'PPE10':self.metrics['PPE10'], 'PPE20':self.metrics['PPE20']}\n",
    "    #     self.logger.experiment.add_scalars('mape', mape_metrics, self.current_epoch)\n",
    "    #     self.logger.experiment.add_scalars('ppe', ppe_metrics, self.current_epoch)\n",
    "    #     self.y_preds = None\n",
    "    #     self.y_true = None\n",
    "    #     return \n",
    "\n",
    "    # def on_test_start(self):\n",
    "    #     self.y_preds = []\n",
    "    #     self.y_true = []\n",
    "    #     self.metrics = None\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     test_loss, y_hat, y, transformer_out, attns, trans_input = self.common_forward(batch, batch_idx)\n",
    "    #     self.log_dict({\"test_loss\": test_loss}, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "    #     self.logger.experiment.add_scalars('loss', {'test': test_loss},self.global_step)\n",
    "    #     self.y_true.extend(y.cpu().numpy().tolist())\n",
    "    #     self.y_preds.extend(y_hat.cpu().numpy().tolist())\n",
    "    #     return test_loss\n",
    "\n",
    "    # def on_test_end(self):\n",
    "    #     assert(len(self.y_preds) == len(self.y_true))\n",
    "    #     self.metrics = self._get_avm_metrics(self.y_true, self.y_preds)\n",
    "    #     print(self.metrics)\n",
    "    #     return \n",
    "    \n",
    "    # Function to get evaluation metrics (MAPE, MdAPE, PPE10, PPE20)\n",
    "    # def _get_avm_metrics(self, y_true, y_pred):\n",
    "    #     y_true = np.exp(np.array(y_true))  # convert back to house prices.\n",
    "    #     y_pred = np.exp(np.array(y_pred))\n",
    "    #     ppe10 = np.count_nonzero((np.abs((np.divide(y_pred, y_true)) - 1) <= 0.1)) / y_pred.size\n",
    "    #     ppe20 = np.count_nonzero((np.abs((np.divide(y_pred, y_true)) - 1) <= 0.2)) / y_pred.size\n",
    "    #     mape  = (np.sum(np.abs(y_true - y_pred) / y_true))/len(y_true)\n",
    "    #     mdape = np.median(np.abs(y_true - y_pred)/y_true)\n",
    "\n",
    "    #     return {'MAPE': mape, 'mdAPE': mdape,\n",
    "    #             'PPE10': ppe10, 'PPE20': ppe20,\n",
    "    #             'Sample_Size': y_true.shape[0]}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.config['learning_rate']\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), betas=config['betas'], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config['lr_gamma'])\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Lightning(config) \n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print('Model has:', int(total_params), 'parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Training\n",
    "#--------------------------------------------------------------------\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=config['save_top_k'],\n",
    "    every_n_train_steps=config['checkpoint_every_n_train_steps'],\n",
    "    save_on_train_epoch_end=True,\n",
    "    monitor = config['monitor'],\n",
    "    mode = config['mode']\n",
    ")\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=config['log_dir'], default_hp_metric=False)\n",
    "\n",
    "print('Using', config['accelerator'])\n",
    "trainer = pl.Trainer(#strategy='ddp_notebook', #strategy='ddp', #'ddp_find_unused_parameters_true', \n",
    "                        accelerator=config['accelerator'], \n",
    "                        devices=config['devices'],\n",
    "                        max_epochs=config['num_epochs'],   \n",
    "                        logger=logger, \n",
    "                        log_every_n_steps=config['log_every_nsteps'], \n",
    "                        callbacks=[checkpoint_callback])   \n",
    "\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader) #, val_dataloaders=test_loader) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
