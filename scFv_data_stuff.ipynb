{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some work on the scFv data and Dataset\n",
    "### The scFv antibody sequence/binding affinity data from a-alpha-Bio's Nature Communications paper\n",
    "quantitative binding scores of scFv-format antibodies against a SARS-CoV-2 target peptide collected via an AlphaSeq assay\n",
    "\n",
    "#### Two datasets:\n",
    "* antibody_dataset_1 : 1109000 MITLL_AAlphaBio_Ab_Binding_dataset.csv rows\n",
    "* antibody_dataset_2 : 1903928 MITLL_AAlphaBio_Ab_Binding_dataset2.csv rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "#### Affinities\n",
    "* reported values are log10(Kd(nM))\n",
    "\n",
    "* delG = RT log10(Kd)    # or -RT log10(Ka)\n",
    "* R = 0.008314 kJ mol-1 (1.98722 cal/K•mol)\n",
    "* T = 298.15 room temp\n",
    "* if Kd = 1.909nM, then delG = -5.16 kcal/mol binding free energy\n",
    "* \"Data include antibodies with predicted affinity measurements ranging from 37 pM to 22 mM\"\n",
    "   * 37pm corresponds to delG = -6.18 kcal/mol binding free energy.\n",
    "   * 22mM corresponds to delG = -0.98 kcal/mol binding free energy.\n",
    "* For reference, the biotin/avidin binding free energy is one of the strongest in nature with absolute free energy of binding, −20.4 kcal/mol\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/fine_tune_config.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(a, **kwargs):\n",
    "    print(kwargs)\n",
    "    result = \"\"\n",
    "    # Iterating over the keys of the Python kwargs dictionary\n",
    "    for arg in kwargs:\n",
    "        result += arg\n",
    "    return result\n",
    "\n",
    "print(concatenate(a=\"Real\", b=\"Python\", c=\"Is\", d=\"Great\", e=\"!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'Sequence']\n",
    "        affinity = self.df.loc[idx, 'Pred_affinity']\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity is negative'\n",
    "        # assert len(seq) < self.config['block_size'], 'sequence is too short'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix now looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            # n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "            # print('n_pred length:', n_pred, ', last_aa - first_aa:', last_aa - first_aa)\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            # masked_idx = torch.randperm(self.config['block_size']-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "            # print('masked_idx:', masked_idx)\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n",
    "\n",
    "\n",
    "        # # get a randomly located block_size-1 substring from the sequence\n",
    "        # # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        # if len(seq) == self.config['block_size']-1:\n",
    "        #     chunk = seq\n",
    "        # else:\n",
    "        #     start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "        #     chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # # encode every character to an integer\n",
    "        # dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # # prepend the CLS token to the sequence\n",
    "        # dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # mask = None\n",
    "        # if self.config['mask_prob'] > 0:\n",
    "        #     # get number of tokens to mask\n",
    "        #     n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        #     # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        #     masked_idx = torch.randperm(self.config['block_size']-1, dtype=torch.long, )[:n_pred]\n",
    "        #     masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "        #     mask = torch.zeros_like(dix)\n",
    "\n",
    "        #     # copy the actual tokens to the mask\n",
    "        #     mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "        #     # ... and overwrite them with MASK token in the data\n",
    "        #     dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv'\n",
    "train_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv'\n",
    "dataset = scFv_Dataset(config, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dix, affinity = dataset.__getitem__(0)\n",
    "print()\n",
    "print(dix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "batch = next(data_iter)\n",
    "dix, affinity = batch\n",
    "print(dix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## antibody_dataset_1\n",
    "### The scFv antibody sequence/binding affinity data in the AlphaSeq experimental assay data\n",
    "\n",
    "#### Some observations:\n",
    "* total rows: 1109000\n",
    "\n",
    "* Number of unique sequences based on actual sequence: 104968\n",
    "* Number of unique sequences using the POI sequence label: 104972\n",
    "* There are between 8-20 instances of each sequence\n",
    "* Number of rows where affinity is not NAN : 340100  (most of original set have nan affinities!??)\n",
    "    * number of unique sequences in this subset: 87211\n",
    "    \n",
    "    * number of unique POIs in this subset: 87215\n",
    "    * for each of these unique sequences, there are multiple binding affinity values\n",
    "         * seq_dict: first entry\n",
    "         \n",
    "         *  seq 1 : , num affinity values: 6 , values: {0.8661294738454064, 0.9084780753465632, 0.9577932321017446, 3.9196288144788087, .....}\n",
    "         *  -RTlog10(3.9) = -4.98 kcal/mol binding free energy\n",
    "         *  -RTlog10(0.866) = -5.36 kcal/mol\n",
    "         *  for reference kT = 0.6 kcal/mol so this difference is on the order of kT\n",
    "    * use the mean value for affinity (or median)?\n",
    "\n",
    "\n",
    "#### The paper:\n",
    "*  https://www.nature.com/articles/s41597-022-01779-4#Tab4 lists 71384 \n",
    "* \"Of the 119,600 designs, 104,972 were successfully built in to the AlphaSeq library and target binding was subsequently measured with 71,384 designs\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc binding free energy (in kcal/mol) from Kd \n",
    "# affinity is Kd in nM\n",
    "R = 1.98722 # cal/(mol*K)\n",
    "T = 298.15 # K\n",
    "def free_energy(Kd):\n",
    "    delG = R * T * math.log10(Kd * 1e-9)    # or -RT log10(Ka)\n",
    "    return delG/1000\n",
    "\n",
    "\n",
    "# def Kd(delG):\n",
    "#     Kd = (10**(-delG*1000/(R * T))) * 1e9\n",
    "#     # Kd = R * T * math.log10(Kd * 1e-9)    # or -RT log10(Ka)\n",
    "#     return Kd  this is bs.!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.592489643"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_energy(100000000) #0.005687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experimental data from AlphaSeq Nature study.\n",
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/MITLL_AAlphaBio_Ab_Binding_dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sequence'][0:10].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sequence'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on distribution of sequence lengths\n",
    "# loop through the rows using iterrows()\n",
    "def get_misc_info(df):\n",
    "    seq_lens = []\n",
    "    seq_set = set()\n",
    "    poi_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        seq = row['Sequence']\n",
    "        poi = row['POI']\n",
    "        seq_lens.append(len(seq))\n",
    "        seq_set.add(seq)\n",
    "        if poi not in poi_dict:\n",
    "            poi_dict[poi] = 1\n",
    "        else:\n",
    "            poi_dict[poi] += 1\n",
    "\n",
    "    print('number of unique sequences:', len(seq_set))\n",
    "    print('number of unique POIs:', len(poi_dict))\n",
    "\n",
    "    counts = list(poi_dict.values())\n",
    "    counts.sort()\n",
    "    print('min count:', counts[0], ', max count:', counts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire antibody_dataset_1 csv file.\n",
    "get_misc_info(df)\n",
    "\n",
    "# results:\n",
    "# number of unique sequences: 104968\n",
    "# number of unique POIs: 104972\n",
    "# min count: 8 , max count: 20\n",
    "#\n",
    "# Note: 104,972 agrees with the number listed in the Nature paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire antibody_dataset_1 csv file.\n",
    "print('starting shape:', df.shape)\n",
    "\n",
    "# Drop NANs in pred affinity column\n",
    "df_clean = df.dropna(subset=['Pred_affinity'])\n",
    "print(df_clean.shape)\n",
    "print('after removing nans:', df_clean.shape)\n",
    "get_misc_info(df_clean)\n",
    "\n",
    "# starting shape: (1109000, 14)\n",
    "# (340100, 14)\n",
    "# after removing nans: (340100, 14)\n",
    "# number of unique sequences: 87211\n",
    "# number of unique POIs: 87215\n",
    "# min count: 1 , max count: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the values for affinity for each repeat of each sequence.\n",
    "seq_lens = []\n",
    "seq_set = set()\n",
    "seq_dict = {} # will contain a set for binding energies for each unique sequence\n",
    "poi_dict = {} # will contain a set for binding energies for each unique POI\n",
    "for index, row in df_clean.iterrows():\n",
    "    seq = row['Sequence']\n",
    "    poi = row['POI']\n",
    "    affinity = row['Pred_affinity']\n",
    "    if seq not in seq_dict:\n",
    "        seq_dict[seq] = set()\n",
    "        seq_dict[seq].add(affinity)\n",
    "    else:\n",
    "        seq_dict[seq].add(affinity)\n",
    "\n",
    "    if poi not in poi_dict:\n",
    "        poi_dict[poi] = set()\n",
    "        poi_dict[poi].add(affinity)\n",
    "    else:\n",
    "        poi_dict[poi].add(affinity)\n",
    "\n",
    "print('len seq_dict:', len(seq_dict))\n",
    "print('len poi_dict:', len(seq_dict))\n",
    "\n",
    "print('seq_dict: first few entries')\n",
    "for i, (k, v) in enumerate(seq_dict.items()):\n",
    "    assert len(v) > 0, 'empty set for sequence'\n",
    "    print(i, ':', ', num values:', len(v), ', values:', v)\n",
    "\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "\n",
    "print('poi_dict: first few entries')\n",
    "for i, (k, v) in enumerate(poi_dict.items()):\n",
    "    assert len(v) > 0, 'empty set for POI'\n",
    "    print(i, ':', ', num values:', len(v), ', values:', v)\n",
    "\n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the number of binding affinity values for the unique sequences (that have binding affinities != NAN)\n",
    "#\n",
    "lengths = [len(v) for k,v in seq_dict.items()]\n",
    "lengths.sort()\n",
    "blah = plt.hist(lengths, bins=20)\n",
    "plt.show()\n",
    "\n",
    "print('number of sequences with just 1 binding affinity value:', int(blah[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Final training set:\n",
    "* 87211 sequences\n",
    "* affinity will be the mean of each sequence’s multiple affinities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len seq_dict:', len(seq_dict))\n",
    "print('seq_dict: first few entries')\n",
    "for i, (k, v) in enumerate(seq_dict.items()):\n",
    "    assert len(v) > 0, 'empty set for sequence'\n",
    "    print(i, ':', ', num values:', len(v), ', values:', v)\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "print()\n",
    "# calc the mean of the binding affinities for each sequence\n",
    "seq_mean_dict = {}\n",
    "for k,v in seq_dict.items():\n",
    "    seq_mean_dict[k] = np.mean(list(v))\n",
    "\n",
    "# print the first few entries\n",
    "for i, (k, v) in enumerate(seq_mean_dict.items()):\n",
    "    print(i, ':', k[:20], ', mean:', v)\n",
    "    if i > 2:\n",
    "        break    \n",
    "\n",
    "lengths = [len(k) for k,v in seq_mean_dict.items()]\n",
    "lengths.sort()\n",
    "print('min length:', lengths[0], ', max length:', lengths[-1])\n",
    "\n",
    "seqs = [k for k,v in seq_mean_dict.items()]\n",
    "s1 = seqs[45]\n",
    "s2 = seqs[2000]\n",
    "diff = [i for i in range(len(s1)) if s1[i] != s2[i]]\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and test sets and save as csv files\n",
    "seqs = [k for k,v in seq_mean_dict.items()]\n",
    "affinities = [v for k,v in seq_mean_dict.items()]\n",
    "\n",
    "df = pd.DataFrame({'Sequence': seqs, 'Pred_affinity': affinities})\n",
    "\n",
    "# Remove enries with negative affinity values\n",
    "df_clean = df[df['Pred_affinity'] > 0]\n",
    "print(df_clean.describe())\n",
    "\n",
    "# Train/test split 90/10\n",
    "# train_df = df_clean.sample(frac=0.9, random_state=42)\n",
    "# test_df = df_clean.drop(train_df.index)\n",
    "# print('train:', train_df.shape, ', test:', test_df.shape)\n",
    "\n",
    "# save to csv\n",
    "# train_df.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv', index=False)\n",
    "# test_df.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(4)\n",
    "print(a.shape)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all rows that contain NAN in the Pred_affinity column.\n",
    "Yikes!  ~75% of the dataframe has NANs in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pred_affinity'].isna().sum()\n",
    "\n",
    "aff = df['Pred_affinity']\n",
    "# c = c[~np.isnan(c)]\n",
    "for i, a in enumerate(aff):\n",
    "    if np.isnan(a):\n",
    "        print(i, a)\n",
    "        break\n",
    "\n",
    "print(df.loc[23925])  #, 'Pred_affinity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aff.shape)\n",
    "aff = aff[~np.isnan(aff)]\n",
    "print(aff.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna(subset=['Pred_affinity'])\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df_clean.sample(frac = 0.10)\n",
    "train_set = df_clean.drop(test_set.index)\n",
    "\n",
    "print(test_set.shape)\n",
    "print(train_set.shape)\n",
    "test_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv')\n",
    "train_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()   #.isinf().sum()\n",
    "\n",
    "# ds = df.isin([np.inf, -np.inf]) \n",
    "# print(ds.shape) \n",
    "  \n",
    "# # printing the count of infinity values \n",
    "# print() \n",
    "# print(\"printing the count of infinity values\") \n",
    "  \n",
    "# count = np.isinf(df).values.sum() \n",
    "# print(\"It contains \" + str(count) + \" infinite values\") \n",
    "  \n",
    "# # counting infinity in a particular column name \n",
    "# c = np.isinf(df['Weight']).values.sum() \n",
    "# print(\"It contains \" + str(c) + \" infinite values\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on distribution of sequence lengths\n",
    "# loop through the rows using iterrows()\n",
    "print(df.shape)\n",
    "seq_lens = []\n",
    "seq_set = set()\n",
    "for index, row in df.iterrows():\n",
    "    seq = row['Sequence']\n",
    "    seq_lens.append(len(seq))\n",
    "    seq_set.add(seq)\n",
    "\n",
    "print(len(seq_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(seq_lens, bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = df.sample(frac = 0.10)\n",
    "# train_set = df.drop(test_set.index)\n",
    "\n",
    "# print(test_set.shape)\n",
    "# print(train_set.shape)\n",
    "# test_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv')\n",
    "# train_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "df = pd.read_csv(data_path, skiprows=6)\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df_clean = df.dropna(subset=['Pred_affinity'])\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = df_2.sample(frac = 0.10)\n",
    "# train_set = df_2.drop(test_set.index)\n",
    "\n",
    "# print(test_set.shape)\n",
    "# print(train_set.shape)\n",
    "\n",
    "# test_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/test_set.csv')\n",
    "# train_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pk.dump(train_set, open('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/train_set.pkl', 'wb'))\n",
    "# pk.dump(test_set, open('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/test_set.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_2.columns.to_list())\n",
    "\n",
    "# s1 = df_2['Sequence'][0]\n",
    "# s2 = df_2['Sequence'][4]\n",
    "# diff = [i for i in range(len(s1)) if s1[i] != s2[i]]\n",
    "# print(diff)\n",
    "\n",
    "# s1 = df_2['Sequence'][0]\n",
    "# s2 = df_2['HC'][4]\n",
    "# print('s1 length:', len(s1), 's2 length:', len(s2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = df_2.iloc[0]\n",
    "# print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### Crafting a dataset for the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FABSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(data_path, skiprows=6)\n",
    "        \n",
    "        # my_set = set()   \n",
    "        # def make_set(x):\n",
    "        #     for c in x:\n",
    "        #         my_set.add(c)\n",
    "\n",
    "        # self.df['Sequence'].apply(make_set)\n",
    "        # self.chars = sorted(list(my_set)) + [\"[MASK]\"]\n",
    "        # print('len of chars:', len(self.chars))\n",
    "        # print('chars:', self.chars)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        self.chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        chunk = self.df.loc[idx, 'Sequence']\n",
    "        \n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "dataset = FABSequenceDataset(config, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.__len__())\n",
    "\n",
    "dix, mask = dataset.__getitem__(0)\n",
    "print(len(dix))\n",
    "print()\n",
    "print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False, num_workers=5, \n",
    "                          pin_memory=True)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "batch = next(data_iter)\n",
    "dix, mask = batch\n",
    "print(dix.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
