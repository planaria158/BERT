{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some work on the scFv data and Dataset\n",
    "### The scFv antibody sequence/binding affinity data from a-alpha-Bio's Nature Communications paper\n",
    "quantitative binding scores of scFv-format antibodies against a SARS-CoV-2 target peptide collected via an AlphaSeq assay\n",
    "\n",
    "#### Two datasets:\n",
    "* antibody_dataset_1 : 1109000 MITLL_AAlphaBio_Ab_Binding_dataset.csv rows\n",
    "* antibody_dataset_2 : 1903928 MITLL_AAlphaBio_Ab_Binding_dataset2.csv rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "#### Affinities\n",
    "* reported values are log10(Kd(nM))\n",
    "\n",
    "* delG = RT log10(Kd)    # or -RT log10(Ka)\n",
    "* R = 0.008314 kJ mol-1 (1.98722 cal/K•mol)\n",
    "* T = 298.15 room temp\n",
    "* if Kd = 1.909nM, then delG = -5.16 kcal/mol binding free energy\n",
    "* \"Data include antibodies with predicted affinity measurements ranging from 37 pM to 22 mM\"\n",
    "   * 37pm corresponds to delG = -6.18 kcal/mol binding free energy.\n",
    "   * 22mM corresponds to delG = -0.98 kcal/mol binding free energy.\n",
    "* For reference, the biotin/avidin binding free energy is one of the strongest in nature with absolute free energy of binding, −20.4 kcal/mol\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regress_head_pdrop': 0.1, 'vocab_size': 23, 'block_size': 91, 'mask_prob': 0.15, 'weight_decay': 0.1, 'learning_rate': 0.0001, 'lr_gamma': 0.9985, 'betas': [0.9, 0.95], 'accelerator': 'gpu', 'devices': 2, 'batch_size': 10, 'num_workers': 20, 'grad_norm_clip': 1.0, 'num_epochs': 10, 'checkpoint_every_n_train_steps': 100, 'save_top_k': 1, 'monitor': 'loss', 'mode': 'min', 'log_dir': './lightning_logs/', 'log_every_nsteps': 100, 'checkpoint_pretrained_bert_name': 'None', 'checkpoint_name': 'None', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/fine_tune_config.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'Sequence']\n",
    "        affinity = self.df.loc[idx, 'Pred_affinity']\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert len(seq) >= self.config['block_size'], 'sequence is too short'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) == self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size']-1, dtype=torch.long, )[:n_pred]\n",
    "        masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite them with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        print('dix shape:', dix.shape, ', affinity:', affinity, ', mask shape:', mask.shape)\n",
    "        \n",
    "        return dix, affinity, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv'\n",
    "train_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv'\n",
    "dataset = scFv_Dataset(config, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.__len__())\n",
    "dix, affinity, mask = dataset.__getitem__(0)\n",
    "print(dix[:10])\n",
    "print(affinity)\n",
    "print(mask[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "batch = next(data_iter)\n",
    "dix, affinity, mask = batch\n",
    "print(dix.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### The scFv antibody sequence/binding affinity data in antibody_dataset_1 the AlphaSeq experimental assay data\n",
    "\n",
    "#### Some observations:\n",
    "* total rows: 1,109,000\n",
    "\n",
    "* Number of unique sequences based on actual sequence: 104,968(huh???)\n",
    "* Number of unique sequences using the POI sequence label: 104,972 ??\n",
    "* There are between 8-20 instances of each sequence\n",
    "* Number of rows where affinity is not a NAN : 340,100  (most of original set have nan affinities!??)\n",
    "    * number of unique sequences: 87211\n",
    "    * number of unique POIs: 87215\n",
    "\n",
    "#### The paper:\n",
    "*  https://www.nature.com/articles/s41597-022-01779-4#Tab4 lists 71,384 \n",
    "* \"Of the 119,600 designs, 104,972 were successfully built in to the AlphaSeq library and target binding was subsequently measured with 71,384 designs\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1109000, 14)\n",
      "Index(['POI', 'Sequence', 'Target', 'Assay', 'Replicate', 'Pred_affinity',\n",
      "       'HC', 'LC', 'CDRH1', 'CDRH2', 'CDRH3', 'CDRL1', 'CDRL2', 'CDRL3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# The experimental data from AlphaSeq Nature study.\n",
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/MITLL_AAlphaBio_Ab_Binding_dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on distribution of sequence lengths\n",
    "# loop through the rows using iterrows()\n",
    "def get_misc_info(df):\n",
    "    seq_lens = []\n",
    "    seq_set = set()\n",
    "    poi_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        seq = row['Sequence']\n",
    "        poi = row['POI']\n",
    "        seq_lens.append(len(seq))\n",
    "        seq_set.add(seq)\n",
    "        if poi not in poi_dict:\n",
    "            poi_dict[poi] = 1\n",
    "        else:\n",
    "            poi_dict[poi] += 1\n",
    "\n",
    "    print('number of unique sequences:', len(seq_set))\n",
    "    print('number of unique POIs:', len(poi_dict))\n",
    "\n",
    "    counts = list(poi_dict.values())\n",
    "    counts.sort()\n",
    "    print('min count:', counts[0], ', max count:', counts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique sequences: 104968\n",
      "number of unique POIs: 104972\n",
      "min count: 8 , max count: 20\n"
     ]
    }
   ],
   "source": [
    "# The entire antibody_dataset_1 csv file.\n",
    "get_misc_info(df)\n",
    "\n",
    "# results:\n",
    "# number of unique sequences: 104968\n",
    "# number of unique POIs: 104972\n",
    "# min count: 8 , max count: 20\n",
    "#\n",
    "# Note: 104,972 agrees with the number listed in the Nature paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting shape: (1109000, 14)\n",
      "(340100, 14)\n",
      "after removing nans: (340100, 14)\n",
      "number of unique sequences: 87211\n",
      "number of unique POIs: 87215\n",
      "min count: 1 , max count: 12\n"
     ]
    }
   ],
   "source": [
    "# The entire antibody_dataset_1 csv file.\n",
    "print('starting shape:', df.shape)\n",
    "# And the set cleaned of NANs\n",
    "df_clean = df.dropna(subset=['Pred_affinity'])\n",
    "print(df_clean.shape)\n",
    "print('after removing nans:', df_clean.shape)\n",
    "get_misc_info(df_clean)\n",
    "\n",
    "# starting shape: (1109000, 14)\n",
    "# (340100, 14)\n",
    "# after removing nans: (340100, 14)\n",
    "# number of unique sequences: 87211\n",
    "# number of unique POIs: 87215\n",
    "# min count: 1 , max count: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len seq_dict: 87211\n",
      "0 : , num values: 6 , values: {0.8661294738454064, 0.9084780753465632, 0.9577932321017446, 3.9196288144788087, 4.914154179746138, 5.723329881345849}\n",
      "1 : , num values: 12 , values: {1.4505434853677084, 1.4248811789665847, 3.736022319355234, 4.208875709332999, 4.1969741379377705, 5.053950729480507, 4.690954801766403, 4.225537246160031, 1.5815625508664832, 4.28826022838054, 4.9744274864843865, 5.007441109453564}\n",
      "2 : , num values: 7 , values: {1.7899955194100752, 2.263831676684976, 3.8100026599168384, 1.7935568167187643, 5.380368626969683, 3.908238706063365, 3.955892163776589}\n",
      "3 : , num values: 7 , values: {1.7042172390142856, 1.7621526510488703, 1.825962828947824, 4.417285434453146, 4.549318549448136, 4.78409345871224, 5.106323925284952}\n",
      "4 : , num values: 8 , values: {1.6254710084772963, 1.6099233031912252, 3.4684917740243204, 4.330292501814343, 1.68272713929683, 3.524101524593373, 4.714744238136452, 3.723603473569288}\n",
      "5 : , num values: 6 , values: {0.4551541513453721, 0.7310596515493035, 0.6008264973911501, 4.527392653894088, 4.36620158798099, 4.911376592348088}\n",
      "6 : , num values: 7 , values: {0.7088969404081613, 0.837280699483502, 0.7957881489955358, 2.793021321788, 4.902049859012823, 4.129883399102775, 4.157092581733772}\n"
     ]
    }
   ],
   "source": [
    "# Examine the values for affinity for each repeat of each sequence.\n",
    "seq_lens = []\n",
    "seq_set = set()\n",
    "seq_dict = {} # will contain a set for binding energies for each unique sequence\n",
    "for index, row in df_clean.iterrows():\n",
    "    seq = row['Sequence']\n",
    "    affinity = row['Pred_affinity']\n",
    "    if seq not in seq_dict:\n",
    "        seq_dict[seq] = set()\n",
    "        seq_dict[seq].add(affinity)\n",
    "    else:\n",
    "        seq_dict[seq].add(affinity)\n",
    "\n",
    "print('len seq_dict:', len(seq_dict))\n",
    "\n",
    "for i, (k, v) in enumerate(seq_dict.items()):\n",
    "    assert len(v) > 0, 'empty set for sequence'\n",
    "    print(i, ':', ', num values:', len(v), ', values:', v)\n",
    "\n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll go forward with the 87,211 sequences that have affinity data as the basis\n",
    "# for my fine-tuning dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all rows that contain NAN in the Pred_affinity column.\n",
    "Yikes!  ~75% of the dataframe has NANs in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pred_affinity'].isna().sum()\n",
    "\n",
    "aff = df['Pred_affinity']\n",
    "# c = c[~np.isnan(c)]\n",
    "for i, a in enumerate(aff):\n",
    "    if np.isnan(a):\n",
    "        print(i, a)\n",
    "        break\n",
    "\n",
    "print(df.loc[23925])  #, 'Pred_affinity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aff.shape)\n",
    "aff = aff[~np.isnan(aff)]\n",
    "print(aff.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna(subset=['Pred_affinity'])\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df_clean.sample(frac = 0.10)\n",
    "train_set = df_clean.drop(test_set.index)\n",
    "\n",
    "print(test_set.shape)\n",
    "print(train_set.shape)\n",
    "test_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv')\n",
    "train_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()   #.isinf().sum()\n",
    "\n",
    "# ds = df.isin([np.inf, -np.inf]) \n",
    "# print(ds.shape) \n",
    "  \n",
    "# # printing the count of infinity values \n",
    "# print() \n",
    "# print(\"printing the count of infinity values\") \n",
    "  \n",
    "# count = np.isinf(df).values.sum() \n",
    "# print(\"It contains \" + str(count) + \" infinite values\") \n",
    "  \n",
    "# # counting infinity in a particular column name \n",
    "# c = np.isinf(df['Weight']).values.sum() \n",
    "# print(\"It contains \" + str(c) + \" infinite values\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on distribution of sequence lengths\n",
    "# loop through the rows using iterrows()\n",
    "print(df.shape)\n",
    "seq_lens = []\n",
    "seq_set = set()\n",
    "for index, row in df.iterrows():\n",
    "    seq = row['Sequence']\n",
    "    seq_lens.append(len(seq))\n",
    "    seq_set.add(seq)\n",
    "\n",
    "print(len(seq_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(seq_lens, bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = df.sample(frac = 0.10)\n",
    "# train_set = df.drop(test_set.index)\n",
    "\n",
    "# print(test_set.shape)\n",
    "# print(train_set.shape)\n",
    "# test_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/test_set.csv')\n",
    "# train_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_1/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "df = pd.read_csv(data_path, skiprows=6)\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df_clean = df.dropna(subset=['Pred_affinity'])\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = df_2.sample(frac = 0.10)\n",
    "# train_set = df_2.drop(test_set.index)\n",
    "\n",
    "# print(test_set.shape)\n",
    "# print(train_set.shape)\n",
    "\n",
    "# test_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/test_set.csv')\n",
    "# train_set.to_csv('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pk.dump(train_set, open('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/train_set.pkl', 'wb'))\n",
    "# pk.dump(test_set, open('./data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/test_set.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_2.columns.to_list())\n",
    "\n",
    "# s1 = df_2['Sequence'][0]\n",
    "# s2 = df_2['Sequence'][4]\n",
    "# diff = [i for i in range(len(s1)) if s1[i] != s2[i]]\n",
    "# print(diff)\n",
    "\n",
    "# s1 = df_2['Sequence'][0]\n",
    "# s2 = df_2['HC'][4]\n",
    "# print('s1 length:', len(s1), 's2 length:', len(s2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = df_2.iloc[0]\n",
    "# print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### Crafting a dataset for the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FABSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(data_path, skiprows=6)\n",
    "        \n",
    "        # my_set = set()   \n",
    "        # def make_set(x):\n",
    "        #     for c in x:\n",
    "        #         my_set.add(c)\n",
    "\n",
    "        # self.df['Sequence'].apply(make_set)\n",
    "        # self.chars = sorted(list(my_set)) + [\"[MASK]\"]\n",
    "        # print('len of chars:', len(self.chars))\n",
    "        # print('chars:', self.chars)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        self.chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        chunk = self.df.loc[idx, 'Sequence']\n",
    "        \n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "dataset = FABSequenceDataset(config, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.__len__())\n",
    "\n",
    "dix, mask = dataset.__getitem__(0)\n",
    "print(len(dix))\n",
    "print()\n",
    "print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False, num_workers=5, \n",
    "                          pin_memory=True)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "batch = next(data_iter)\n",
    "dix, mask = batch\n",
    "print(dix.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
