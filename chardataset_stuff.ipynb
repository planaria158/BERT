{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/default.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "        chars = sorted(list(set(data))) + [\"[MASK]\"] # + mask token\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.config['block_size']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return None, None\n",
    "        # # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        # # encode every character to an integer\n",
    "        # dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        # mask = torch.zeros_like(dix)\n",
    "        # mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask # Return batch + mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('./minBERT/input.txt', 'r').read() \n",
    "train_dataset = CharDataset(config, text)\n",
    "print(train_dataset.__len__())\n",
    "dix, mask = train_dataset.__getitem__(0)\n",
    "print(dix)\n",
    "print()\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup the dataloader\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     sampler=torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=int(1e10)),\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     batch_size=config['batch_size'],\n",
    "#     num_workers=config['num_workers'],\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=5, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "# fetch the next batch (x, y) and re-init iterator if needed\n",
    "# try:\n",
    "batch = next(data_iter)\n",
    "# except StopIteration:\n",
    "#     data_iter = iter(train_loader)\n",
    "#     batch = next(data_iter)\n",
    "# batch = [t.to(self.device) for t in batch]\n",
    "# x, mask = batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
