{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some work on the OAS data and dataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "# pd.options.mode.copy_on_write = True # to avoid SettingWithCopyWarning\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.core import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp_dropout': 0.1, 'vocab_size': 23, 'block_size': 91, 'weight_decay': 0.1, 'learning_rate': 0.0001, 'lr_gamma': 0.9985, 'betas': [0.9, 0.95], 'accelerator': 'gpu', 'devices': 2, 'batch_size': 120, 'num_workers': 20, 'grad_norm_clip': 1.0, 'num_epochs': 10, 'checkpoint_every_n_train_steps': 100, 'save_top_k': 2, 'monitor': 'loss', 'mode': 'min', 'log_dir': './lightning_logs/', 'log_every_nsteps': 100, 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Read the config\n",
    "#\n",
    "config_path = './config/pretrain_config.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### OAS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/mark/dev/myBERT/data/oas/human_light_sars_covid/1279049_1_Light_Bulk.csv', skiprows=1)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# Note, v_, d_, j_, c_ are the columns for the VDJC gene sequences : variable, diverse, joining, constant\n",
    "# Antibodies obtain their diversity through 2 processes. The first is called V(D)J (variable, diverse, and joining regions) \n",
    "# recombination. During cell maturation, the B cell splices out the DNA of all but one of the genes from each region \n",
    "# and combine the three remaining genes to form one VDJ segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['sequence'][0])\n",
    "print(df['locus'][0])\n",
    "print(df['v_frameshift'][0])\n",
    "print(df['sequence_alignment'][0])\n",
    "print(df['sequence_alignment_aa'][0])\n",
    "print(df['fwr1_start'][0])\n",
    "print(df['fwr1_end'][0])\n",
    "print(df['fwr1'][0], ', len(fwr1) = ', len(df['fwr1'][0]))\n",
    "print(df['Redundancy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the amino acid sequences\n",
    "cols = df.columns.to_list()\n",
    "# print(cols)\n",
    "aa_cols = [c for c in cols if 'aa' in c]\n",
    "for a in aa_cols:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in aa_cols:\n",
    "    s = df[col][0]\n",
    "    if isinstance(s, str):\n",
    "        print(col, ', ', len(s), ', ', s[:10])\n",
    "    else:\n",
    "        print(col, ', ', s)\n",
    "\n",
    "s1 = df['sequence_alignment_aa'][0]\n",
    "s2 = df['germline_alignment_aa'][0]\n",
    "diff = [i for i in range(len(s1)) if s1[i] != s2[i]]\n",
    "print('diff:', diff)\n",
    "print(s1[diff[0]-3:diff[0]+3], ', ', s2[diff[0]-3:diff[0]+3])\n",
    "\n",
    "s1 = df['sequence_alignment_aa'][0]\n",
    "s2 = df['fwr1'][0]\n",
    "print((s2 in s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "### Plan\n",
    "* focus on just sequence_alignment_aa column (not germline)\n",
    "    * eliminate duplicates\n",
    "* ignore the separate v, d, and j (they are already in the light chain)\n",
    "* ignore fwr1, fwr2, fwr3, fwr4 regions (on the heavy chain?)\n",
    "* ignore cdr sequences (they are already contained in the longer light-chain sequences)\n",
    "\n",
    "### Result of data extraction\n",
    "* total rows 20306305,  num unique, len(seqs): 18061315\n",
    "* range in length from min_len: 43 , max_len: 132\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all .csv files in a directory\n",
    "# extract just the sequence_alignment_aa column data\n",
    "# put into a set (to remove duplicates)\n",
    "# There are 175 csv files in the directory\n",
    "\n",
    "import glob\n",
    "files = glob.glob('/home/mark/dev/myBERT/data/oas/human_light_sars_covid/*.csv')\n",
    "print(len(files), 'csv files to process')\n",
    "seqs = set()\n",
    "total_rows = 0\n",
    "for i, file in enumerate(files):\n",
    "    df = pd.read_csv(file, skiprows=1)\n",
    "    total_rows += df.shape[0]\n",
    "    print(i, ':', os.path.basename(file), ', num rows:' , df.shape[0])\n",
    "    seqs.update(df['sequence_alignment_aa'].to_list())\n",
    "    print('\\ttotal rows so far:', total_rows, ', num unique, len(seqs):', len(seqs))\n",
    "\n",
    "\n",
    "\n",
    "max_len = -99\n",
    "min_len = 100000\n",
    "print(len(seqs), 'unique sequences')\n",
    "for i, s in enumerate(seqs):\n",
    "    if i < 10: \n",
    "        print(len(s), ', ', s)\n",
    "\n",
    "    if len(s) < min_len:\n",
    "        min_len = len(s)\n",
    "    elif len(s) > max_len:\n",
    "        max_len = len(s)\n",
    "\n",
    "print('min_len:', min_len, ', max_len:', max_len)\n",
    "\n",
    "# pk.dump(seqs, open('/home/mark/dev/myBERT/data/oas/human_light_sars_covid/unique_seqs.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the string lengths in the seqs set\n",
    "lengths = [len(s) for s in seqs]\n",
    "\n",
    "\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_trimmed = seqs.copy()\n",
    "\n",
    "# remove all sequencs less than 90 in length\n",
    "for s in seqs:\n",
    "    if len(s) < 90:\n",
    "        seqs_trimmed.remove(s)\n",
    "\n",
    "\n",
    "print(len(seqs_trimmed), 'unique sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = -99\n",
    "min_len = 100000\n",
    "print(len(seqs_trimmed), 'unique sequences')\n",
    "for i, s in enumerate(seqs_trimmed):\n",
    "    if len(s) < min_len:\n",
    "        min_len = len(s)\n",
    "    elif len(s) > max_len:\n",
    "        max_len = len(s)\n",
    "\n",
    "print('min_len:', min_len, ', max_len:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the string lengths in the seqs set\n",
    "lengths = [len(s) for s in seqs_trimmed]\n",
    "\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/test split\n",
    "data = list(seqs_trimmed)\n",
    "np.random.shuffle(data)\n",
    "split = int(0.8 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]\n",
    "\n",
    "# pk.dump(train_data, open('/home/mark/dev/myBERT/data/oas/human_light_sars_covid/train_data.pk', 'wb'))\n",
    "# pk.dump(test_data, open('/home/mark/dev/myBERT/data/oas/human_light_sars_covid/test_data.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), ', ', len(test_data))\n",
    "\n",
    "max_len = -99\n",
    "min_len = 100000\n",
    "for i, s in enumerate(train_data):\n",
    "    if len(s) < min_len:\n",
    "        min_len = len(s)\n",
    "    elif len(s) > max_len:\n",
    "        max_len = len(s)\n",
    "\n",
    "print('min_len:', min_len, ', max_len:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in test_data[0:3]:\n",
    "    print(seq, ', len(seq):', len(seq))\n",
    "    print('\\tlen(seq) - self.config[block_size]:', len(seq) - config['block_size'])\n",
    "    start_idx = np.random.randint(0, len(seq) - config['block_size'])\n",
    "    print('\\tstart_idx:', start_idx)\n",
    "    chunk = seq[start_idx:start_idx + config['block_size']]\n",
    "    print('\\tlen(chunk):', len(chunk), ', chunk:', chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Dataset for OAS data\n",
    "#--------------------------------------------------------\n",
    "class OASSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits sequences of aa's from the OAS data\n",
    "    \"\"\"\n",
    "    def __init__(self, config, pk_file_path):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print('reading the data from:', pk_file_path)\n",
    "        pk_data = pk.load(open(pk_file_path, 'rb'))\n",
    "        self.data = list(pk_data)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = len(self.data), len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) == self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size']-1, dtype=torch.long, )[:n_pred]\n",
    "        masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '/home/mark/dev/myBERT/data/oas/human_light_sars_covid/test_data.pk'\n",
    "test_dataset = OASSequenceDataset(config, test_data_path)\n",
    "print(test_dataset.__len__())\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=config['batch_size'], num_workers=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dix, mask = test_dataset.__getitem__(0)\n",
    "print()\n",
    "print(dix)\n",
    "print()\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ = iter(test_loader)\n",
    "dix, mask = next(iter_)\n",
    "print(dix.shape, mask.shape)\n",
    "print(dix[0])\n",
    "print()\n",
    "print(mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(['CLS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list([\"CLS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddling with perplexity metric\n",
    "import torch\n",
    "from torcheval.metrics.text import Perplexity\n",
    "\n",
    "metric=Perplexity()\n",
    "input = torch.tensor([[[0.3659, 0.7025, 0.3104]], [[0.0097, 0.6577, 0.1947]],[[0.5659, 0.0025, 0.0104]], [[0.9097, 0.0577, 0.7947]]])\n",
    "target = torch.tensor([[2],  [1], [2],  [1]])\n",
    "print('input shape:', input.shape)\n",
    "print('target shape:', target.shape)\n",
    "\n",
    "metric.update(input, target)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### Crafting a dataset for the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FABSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(data_path, skiprows=6)\n",
    "        \n",
    "        # my_set = set()   \n",
    "        # def make_set(x):\n",
    "        #     for c in x:\n",
    "        #         my_set.add(c)\n",
    "\n",
    "        # self.df['Sequence'].apply(make_set)\n",
    "        # self.chars = sorted(list(my_set)) + [\"[MASK]\"]\n",
    "        # print('len of chars:', len(self.chars))\n",
    "        # print('chars:', self.chars)\n",
    "    \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token\n",
    "        self.chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '[MASK]']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size) characters from the data\n",
    "        # chunk = self.data[idx:idx + self.config['block_size']]\n",
    "        chunk = self.df.loc[idx, 'Sequence']\n",
    "        \n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # get number of tokens to mask\n",
    "        n_pred = max(1, int(round(self.config['block_size']*self.config['mask_prob'])))\n",
    "\n",
    "        # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "        masked_idx = torch.randperm(self.config['block_size'], dtype=torch.long, )[:n_pred]\n",
    "\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # copy the actual tokens to the mask\n",
    "        mask[masked_idx] = dix[masked_idx]\n",
    "        \n",
    "        # ... and overwrite then with MASK token in the data\n",
    "        dix[masked_idx] = self.stoi[\"[MASK]\"]\n",
    "\n",
    "        return dix, mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mit-ll/mit-ll-AlphaSeq_Antibody_Dataset-a8f64a9/antibody_dataset_2/MITLL_AAlphaBio_Ab_Binding_dataset2.csv'\n",
    "dataset = FABSequenceDataset(config, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.__len__())\n",
    "\n",
    "dix, mask = dataset.__getitem__(0)\n",
    "print(len(dix))\n",
    "print()\n",
    "print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False, num_workers=5, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(data_iter)\n",
    "dix, mask = batch\n",
    "print(dix.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
